{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your torch version is 1.13.1 which does not support torch.compile\n",
      "proceeding without onnxruntime\n",
      "Torch TensorRT not enabled\n"
     ]
    }
   ],
   "source": [
    "import handler  # Assumes handler.py is in the same directory\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "class Context:\n",
    "    # Mock context object for testing\n",
    "    system_properties = {\n",
    "        \"model_dir\": \"/Users/lhe339/Documents/GitHub/TTS/saved_models\"  # Update this path\n",
    "    }\n",
    "\n",
    "    manifest = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a mock context object\n",
    "context = Context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a handler object\n",
    "handler_obj = handler.SpeechSynthesisHandler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type hifigan to instantiate a model of type speecht5. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SpeechT5ForTextToSpeech were not initialized from the model checkpoint at /Users/lhe339/Documents/GitHub/TTS/saved_models and are newly initialized: ['encoder.wrapped_encoder.layers.7.attention.out_proj.bias', 'decoder.wrapped_decoder.layers.3.encoder_attn.q_proj.bias', 'encoder.wrapped_encoder.layers.3.layer_norm.weight', 'encoder.wrapped_encoder.layers.9.attention.q_proj.weight', 'decoder.wrapped_decoder.layers.2.self_attn.q_proj.weight', 'decoder.wrapped_decoder.layers.4.encoder_attn.out_proj.weight', 'decoder.wrapped_decoder.layers.4.self_attn.k_proj.weight', 'encoder.wrapped_encoder.layers.7.attention.q_proj.weight', 'decoder.wrapped_decoder.layers.1.encoder_attn_layer_norm.bias', 'encoder.wrapped_encoder.layers.8.attention.k_proj.bias', 'decoder.wrapped_decoder.layers.4.encoder_attn.q_proj.weight', 'decoder.wrapped_decoder.layers.5.self_attn.out_proj.weight', 'encoder.wrapped_encoder.layers.10.attention.q_proj.bias', 'decoder.wrapped_decoder.layers.3.encoder_attn.k_proj.weight', 'decoder.wrapped_decoder.layers.5.self_attn_layer_norm.weight', 'speech_decoder_postnet.layers.1.batch_norm.num_batches_tracked', 'speech_decoder_postnet.layers.2.batch_norm.bias', 'encoder.wrapped_encoder.layers.6.layer_norm.bias', 'encoder.wrapped_encoder.layers.5.attention.q_proj.bias', 'encoder.wrapped_encoder.layers.6.attention.v_proj.bias', 'encoder.wrapped_encoder.layers.4.attention.k_proj.bias', 'decoder.wrapped_decoder.layers.3.self_attn_layer_norm.bias', 'decoder.wrapped_decoder.layers.0.encoder_attn.k_proj.bias', 'encoder.wrapped_encoder.layers.11.attention.q_proj.bias', 'speech_decoder_postnet.layers.2.batch_norm.weight', 'decoder.wrapped_decoder.layers.0.final_layer_norm.weight', 'encoder.wrapped_encoder.layers.2.feed_forward.output_dense.weight', 'decoder.wrapped_decoder.layers.1.self_attn.out_proj.weight', 'decoder.wrapped_decoder.layers.3.feed_forward.intermediate_dense.bias', 'decoder.prenet.layers.1.weight', 'decoder.prenet.layers.0.bias', 'encoder.wrapped_encoder.layers.3.final_layer_norm.weight', 'decoder.wrapped_decoder.layers.2.self_attn.q_proj.bias', 'encoder.wrapped_encoder.layers.3.feed_forward.output_dense.weight', 'decoder.wrapped_decoder.layers.2.encoder_attn.q_proj.weight', 'encoder.wrapped_encoder.layers.11.attention.out_proj.weight', 'decoder.wrapped_decoder.layers.0.encoder_attn.q_proj.bias', 'encoder.wrapped_encoder.layers.10.feed_forward.intermediate_dense.bias', 'decoder.wrapped_decoder.layers.5.self_attn.q_proj.weight', 'decoder.wrapped_decoder.layers.3.feed_forward.intermediate_dense.weight', 'encoder.wrapped_encoder.layers.5.attention.out_proj.weight', 'encoder.wrapped_encoder.layers.1.feed_forward.intermediate_dense.weight', 'encoder.wrapped_encoder.layers.7.feed_forward.intermediate_dense.bias', 'encoder.wrapped_encoder.layers.7.feed_forward.output_dense.bias', 'decoder.wrapped_decoder.layers.5.encoder_attn.k_proj.bias', 'encoder.wrapped_encoder.layers.6.feed_forward.intermediate_dense.bias', 'encoder.wrapped_encoder.layers.6.final_layer_norm.weight', 'encoder.wrapped_encoder.layers.2.feed_forward.intermediate_dense.bias', 'encoder.wrapped_encoder.layers.7.attention.k_proj.weight', 'decoder.wrapped_decoder.layers.4.self_attn.v_proj.weight', 'decoder.wrapped_decoder.layers.5.encoder_attn_layer_norm.bias', 'encoder.wrapped_encoder.layers.10.feed_forward.output_dense.bias', 'decoder.wrapped_decoder.layers.4.encoder_attn.v_proj.weight', 'decoder.wrapped_decoder.layers.5.encoder_attn.v_proj.bias', 'decoder.wrapped_decoder.layers.1.encoder_attn.q_proj.weight', 'encoder.wrapped_encoder.layers.6.attention.k_proj.bias', 'encoder.wrapped_encoder.layers.11.feed_forward.output_dense.bias', 'decoder.wrapped_decoder.layers.1.encoder_attn_layer_norm.weight', 'encoder.wrapped_encoder.layers.10.attention.v_proj.bias', 'decoder.wrapped_decoder.layers.1.feed_forward.output_dense.weight', 'decoder.wrapped_decoder.layers.2.self_attn_layer_norm.weight', 'decoder.wrapped_decoder.layers.5.self_attn.k_proj.bias', 'decoder.wrapped_decoder.layers.5.feed_forward.intermediate_dense.weight', 'encoder.wrapped_encoder.layers.2.feed_forward.intermediate_dense.weight', 'speech_decoder_postnet.layers.3.batch_norm.num_batches_tracked', 'decoder.prenet.speaker_embeds_layer.weight', 'encoder.wrapped_encoder.layers.6.feed_forward.intermediate_dense.weight', 'encoder.wrapped_encoder.layers.3.attention.v_proj.weight', 'decoder.wrapped_decoder.layers.0.self_attn.out_proj.bias', 'decoder.wrapped_decoder.layers.2.encoder_attn.k_proj.bias', 'encoder.wrapped_encoder.layers.4.final_layer_norm.bias', 'encoder.wrapped_encoder.layers.1.layer_norm.weight', 'speech_decoder_postnet.layers.0.conv.weight', 'speech_decoder_postnet.layers.1.batch_norm.running_mean', 'encoder.wrapped_encoder.layers.10.attention.k_proj.weight', 'decoder.wrapped_decoder.layers.3.encoder_attn.v_proj.weight', 'speech_decoder_postnet.layers.0.batch_norm.running_mean', 'encoder.wrapped_encoder.layers.0.layer_norm.bias', 'decoder.wrapped_decoder.layers.0.encoder_attn.q_proj.weight', 'encoder.prenet.encode_positions.alpha', 'decoder.wrapped_decoder.layers.0.self_attn.out_proj.weight', 'decoder.wrapped_decoder.layers.4.feed_forward.output_dense.bias', 'decoder.wrapped_decoder.layers.0.encoder_attn_layer_norm.weight', 'encoder.wrapped_encoder.layer_norm.weight', 'encoder.wrapped_encoder.layers.11.attention.v_proj.weight', 'speech_decoder_postnet.layers.4.batch_norm.weight', 'decoder.wrapped_decoder.layers.4.encoder_attn.q_proj.bias', 'encoder.wrapped_encoder.layers.11.final_layer_norm.bias', 'decoder.wrapped_decoder.layers.3.final_layer_norm.weight', 'encoder.wrapped_encoder.layers.5.attention.q_proj.weight', 'decoder.wrapped_decoder.layers.5.encoder_attn.out_proj.weight', 'decoder.wrapped_decoder.layers.4.self_attn_layer_norm.bias', 'encoder.wrapped_encoder.layers.2.attention.out_proj.weight', 'encoder.wrapped_encoder.layers.2.attention.v_proj.weight', 'decoder.wrapped_decoder.layers.1.feed_forward.output_dense.bias', 'decoder.wrapped_decoder.layers.5.self_attn.v_proj.bias', 'encoder.wrapped_encoder.layers.4.attention.out_proj.bias', 'decoder.wrapped_decoder.layers.2.self_attn.out_proj.bias', 'decoder.wrapped_decoder.layers.4.encoder_attn.out_proj.bias', 'encoder.wrapped_encoder.layers.6.feed_forward.output_dense.bias', 'decoder.wrapped_decoder.layers.2.feed_forward.intermediate_dense.bias', 'encoder.wrapped_encoder.layers.10.attention.q_proj.weight', 'decoder.wrapped_decoder.layers.2.encoder_attn.q_proj.bias', 'encoder.wrapped_encoder.layers.2.attention.k_proj.bias', 'speech_decoder_postnet.layers.1.conv.weight', 'encoder.wrapped_encoder.layers.8.attention.k_proj.weight', 'decoder.wrapped_decoder.layers.3.feed_forward.output_dense.weight', 'encoder.wrapped_encoder.layers.7.final_layer_norm.weight', 'decoder.wrapped_decoder.layers.1.self_attn.k_proj.weight', 'encoder.wrapped_encoder.layers.1.feed_forward.output_dense.weight', 'decoder.wrapped_decoder.layers.4.self_attn.q_proj.bias', 'encoder.wrapped_encoder.layers.10.final_layer_norm.weight', 'decoder.wrapped_decoder.layers.5.feed_forward.output_dense.weight', 'encoder.wrapped_encoder.layers.10.attention.k_proj.bias', 'decoder.prenet.layers.1.bias', 'decoder.wrapped_decoder.layers.5.final_layer_norm.weight', 'speech_decoder_postnet.layers.4.batch_norm.running_var', 'decoder.wrapped_decoder.layers.3.feed_forward.output_dense.bias', 'encoder.wrapped_encoder.layers.1.final_layer_norm.bias', 'decoder.wrapped_decoder.layers.0.encoder_attn.out_proj.bias', 'decoder.wrapped_decoder.layers.4.encoder_attn_layer_norm.bias', 'encoder.wrapped_encoder.layers.5.feed_forward.output_dense.bias', 'decoder.wrapped_decoder.layers.4.feed_forward.intermediate_dense.weight', 'decoder.wrapped_decoder.layers.5.encoder_attn.out_proj.bias', 'decoder.wrapped_decoder.layers.4.final_layer_norm.bias', 'decoder.wrapped_decoder.layers.0.final_layer_norm.bias', 'speech_decoder_postnet.layers.0.batch_norm.bias', 'decoder.wrapped_decoder.layers.4.self_attn_layer_norm.weight', 'encoder.wrapped_encoder.layers.1.feed_forward.output_dense.bias', 'decoder.wrapped_decoder.layers.2.encoder_attn.v_proj.weight', 'decoder.prenet.encode_positions.alpha', 'speech_decoder_postnet.layers.3.batch_norm.weight', 'decoder.wrapped_decoder.layers.0.feed_forward.intermediate_dense.weight', 'decoder.wrapped_decoder.layers.1.self_attn.q_proj.weight', 'encoder.wrapped_encoder.layers.11.attention.k_proj.weight', 'encoder.wrapped_encoder.layers.7.attention.out_proj.weight', 'speech_decoder_postnet.layers.2.batch_norm.running_var', 'encoder.wrapped_encoder.layers.4.feed_forward.intermediate_dense.bias', 'encoder.wrapped_encoder.layers.8.feed_forward.output_dense.weight', 'encoder.wrapped_encoder.layers.9.final_layer_norm.weight', 'decoder.wrapped_decoder.layers.1.self_attn_layer_norm.bias', 'encoder.wrapped_encoder.layers.0.attention.out_proj.weight', 'encoder.wrapped_encoder.layers.8.final_layer_norm.weight', 'encoder.wrapped_encoder.layers.2.attention.out_proj.bias', 'encoder.wrapped_encoder.layers.8.attention.out_proj.bias', 'decoder.wrapped_decoder.layers.0.self_attn.q_proj.bias', 'encoder.wrapped_encoder.layers.0.attention.q_proj.bias', 'decoder.wrapped_decoder.layers.1.encoder_attn.q_proj.bias', 'encoder.wrapped_encoder.layers.5.feed_forward.intermediate_dense.bias', 'encoder.wrapped_encoder.layers.8.attention.v_proj.bias', 'decoder.wrapped_decoder.layers.5.self_attn.out_proj.bias', 'decoder.wrapped_decoder.layers.0.encoder_attn.v_proj.bias', 'encoder.wrapped_encoder.layers.0.attention.v_proj.weight', 'encoder.wrapped_encoder.layers.0.attention.out_proj.bias', 'decoder.wrapped_decoder.layers.0.self_attn.v_proj.weight', 'encoder.wrapped_encoder.layers.8.attention.q_proj.weight', 'decoder.wrapped_decoder.layers.5.encoder_attn.v_proj.weight', 'decoder.wrapped_decoder.layers.1.encoder_attn.v_proj.bias', 'decoder.wrapped_decoder.layers.2.encoder_attn.k_proj.weight', 'decoder.wrapped_decoder.layers.2.self_attn.k_proj.bias', 'decoder.wrapped_decoder.layers.4.encoder_attn.k_proj.weight', 'decoder.wrapped_decoder.layers.3.encoder_attn_layer_norm.weight', 'encoder.wrapped_encoder.layers.6.layer_norm.weight', 'encoder.wrapped_encoder.layers.9.final_layer_norm.bias', 'decoder.wrapped_decoder.layers.0.self_attn.k_proj.weight', 'encoder.wrapped_encoder.layer_norm.bias', 'decoder.prenet.final_layer.bias', 'encoder.wrapped_encoder.layers.3.feed_forward.output_dense.bias', 'encoder.wrapped_encoder.layers.8.layer_norm.bias', 'encoder.wrapped_encoder.layers.1.attention.k_proj.bias', 'speech_decoder_postnet.layers.1.batch_norm.weight', 'encoder.wrapped_encoder.layers.0.attention.v_proj.bias', 'decoder.wrapped_decoder.layers.1.encoder_attn.k_proj.bias', 'decoder.wrapped_decoder.layers.0.self_attn.q_proj.weight', 'encoder.wrapped_encoder.layers.5.attention.k_proj.weight', 'speech_decoder_postnet.layers.4.batch_norm.running_mean', 'encoder.wrapped_encoder.layers.0.attention.k_proj.weight', 'decoder.wrapped_decoder.layers.5.self_attn_layer_norm.bias', 'decoder.wrapped_decoder.layers.2.self_attn_layer_norm.bias', 'decoder.wrapped_decoder.layers.4.self_attn.v_proj.bias', 'encoder.wrapped_encoder.layers.0.feed_forward.output_dense.weight', 'speech_decoder_postnet.layers.0.batch_norm.running_var', 'encoder.wrapped_encoder.layers.9.attention.v_proj.bias', 'speech_decoder_postnet.layers.2.batch_norm.num_batches_tracked', 'encoder.wrapped_encoder.layers.4.layer_norm.weight', 'encoder.wrapped_encoder.layers.1.attention.q_proj.bias', 'encoder.wrapped_encoder.layers.1.layer_norm.bias', 'decoder.wrapped_decoder.layers.4.encoder_attn_layer_norm.weight', 'encoder.wrapped_encoder.layers.8.attention.out_proj.weight', 'encoder.wrapped_encoder.layers.6.attention.out_proj.bias', 'encoder.wrapped_encoder.layers.9.feed_forward.output_dense.weight', 'decoder.wrapped_decoder.layers.5.encoder_attn.k_proj.weight', 'encoder.wrapped_encoder.layers.7.attention.k_proj.bias', 'encoder.wrapped_encoder.layers.8.feed_forward.output_dense.bias', 'decoder.wrapped_decoder.layers.3.encoder_attn.out_proj.weight', 'encoder.wrapped_encoder.layers.5.attention.v_proj.bias', 'encoder.wrapped_encoder.layers.9.attention.v_proj.weight', 'decoder.wrapped_decoder.layers.4.self_attn.out_proj.weight', 'decoder.wrapped_decoder.layers.0.self_attn_layer_norm.bias', 'decoder.wrapped_decoder.layers.2.encoder_attn.v_proj.bias', 'decoder.wrapped_decoder.layers.3.encoder_attn.k_proj.bias', 'decoder.wrapped_decoder.layers.3.encoder_attn.q_proj.weight', 'encoder.wrapped_encoder.layers.8.attention.q_proj.bias', 'encoder.wrapped_encoder.layers.6.feed_forward.output_dense.weight', 'decoder.wrapped_decoder.layers.5.final_layer_norm.bias', 'encoder.wrapped_encoder.layers.6.attention.k_proj.weight', 'encoder.wrapped_encoder.layers.4.attention.v_proj.bias', 'encoder.wrapped_encoder.layers.7.layer_norm.weight', 'decoder.wrapped_decoder.layers.2.feed_forward.intermediate_dense.weight', 'decoder.wrapped_decoder.layers.0.encoder_attn.k_proj.weight', 'encoder.wrapped_encoder.layers.3.feed_forward.intermediate_dense.bias', 'decoder.wrapped_decoder.layers.2.feed_forward.output_dense.weight', 'encoder.wrapped_encoder.layers.6.attention.q_proj.bias', 'decoder.wrapped_decoder.layers.1.final_layer_norm.weight', 'decoder.wrapped_decoder.layers.0.feed_forward.intermediate_dense.bias', 'encoder.wrapped_encoder.layers.1.attention.k_proj.weight', 'encoder.wrapped_encoder.layers.5.attention.out_proj.bias', 'encoder.wrapped_encoder.layers.4.attention.out_proj.weight', 'decoder.wrapped_decoder.layers.3.self_attn.out_proj.weight', 'speech_decoder_postnet.layers.2.conv.weight', 'encoder.wrapped_encoder.layers.0.feed_forward.intermediate_dense.bias', 'decoder.wrapped_decoder.layers.2.self_attn.out_proj.weight', 'encoder.wrapped_encoder.layers.2.attention.v_proj.bias', 'speech_decoder_postnet.layers.1.batch_norm.bias', 'decoder.wrapped_decoder.layers.1.encoder_attn.v_proj.weight', 'encoder.wrapped_encoder.layers.5.layer_norm.bias', 'decoder.wrapped_decoder.layers.1.self_attn.out_proj.bias', 'encoder.wrapped_encoder.layers.8.feed_forward.intermediate_dense.weight', 'encoder.wrapped_encoder.layers.0.attention.q_proj.weight', 'encoder.wrapped_encoder.layers.11.attention.k_proj.bias', 'encoder.wrapped_encoder.layers.5.attention.k_proj.bias', 'encoder.wrapped_encoder.layers.0.feed_forward.intermediate_dense.weight', 'encoder.wrapped_encoder.layers.11.attention.v_proj.bias', 'encoder.wrapped_encoder.layers.11.feed_forward.intermediate_dense.weight', 'encoder.wrapped_encoder.layers.4.layer_norm.bias', 'decoder.wrapped_decoder.layers.4.feed_forward.intermediate_dense.bias', 'decoder.wrapped_decoder.layers.1.feed_forward.intermediate_dense.bias', 'encoder.wrapped_encoder.layers.1.attention.out_proj.weight', 'encoder.wrapped_encoder.layers.10.feed_forward.intermediate_dense.weight', 'decoder.wrapped_decoder.layers.5.self_attn.k_proj.weight', 'speech_decoder_postnet.prob_out.bias', 'decoder.wrapped_decoder.layers.5.encoder_attn.q_proj.weight', 'encoder.wrapped_encoder.layers.8.layer_norm.weight', 'speech_decoder_postnet.prob_out.weight', 'encoder.wrapped_encoder.layers.10.layer_norm.weight', 'encoder.wrapped_encoder.layers.2.feed_forward.output_dense.bias', 'encoder.wrapped_encoder.layers.9.feed_forward.intermediate_dense.bias', 'encoder.wrapped_encoder.layers.2.final_layer_norm.weight', 'encoder.wrapped_encoder.layers.6.final_layer_norm.bias', 'encoder.wrapped_encoder.layers.9.attention.out_proj.bias', 'encoder.wrapped_encoder.layers.5.final_layer_norm.bias', 'decoder.wrapped_decoder.layers.0.self_attn.k_proj.bias', 'encoder.wrapped_encoder.layers.10.attention.out_proj.bias', 'encoder.wrapped_encoder.layers.6.attention.q_proj.weight', 'encoder.wrapped_encoder.layers.1.final_layer_norm.weight', 'encoder.wrapped_encoder.layers.10.attention.out_proj.weight', 'decoder.prenet.speaker_embeds_layer.bias', 'encoder.wrapped_encoder.layers.3.attention.v_proj.bias', 'encoder.wrapped_encoder.layers.0.feed_forward.output_dense.bias', 'decoder.wrapped_decoder.layers.2.self_attn.v_proj.weight', 'decoder.wrapped_decoder.layers.1.self_attn.v_proj.weight', 'encoder.wrapped_encoder.layers.2.layer_norm.weight', 'decoder.wrapped_decoder.layers.0.encoder_attn.v_proj.weight', 'decoder.wrapped_decoder.layers.3.self_attn.v_proj.weight', 'decoder.wrapped_decoder.layers.3.self_attn.k_proj.bias', 'decoder.wrapped_decoder.layers.3.self_attn_layer_norm.weight', 'encoder.wrapped_encoder.layers.9.attention.k_proj.bias', 'encoder.wrapped_encoder.layers.1.feed_forward.intermediate_dense.bias', 'decoder.wrapped_decoder.layers.2.final_layer_norm.bias', 'decoder.wrapped_decoder.layers.0.encoder_attn.out_proj.weight', 'decoder.wrapped_decoder.layers.1.feed_forward.intermediate_dense.weight', 'encoder.wrapped_encoder.layers.6.attention.v_proj.weight', 'speech_decoder_postnet.layers.4.conv.weight', 'decoder.wrapped_decoder.layers.2.encoder_attn.out_proj.bias', 'encoder.wrapped_encoder.layers.3.feed_forward.intermediate_dense.weight', 'encoder.wrapped_encoder.layers.7.layer_norm.bias', 'decoder.wrapped_decoder.layers.3.encoder_attn.v_proj.bias', 'decoder.prenet.layers.0.weight', 'encoder.wrapped_encoder.layers.4.final_layer_norm.weight', 'encoder.wrapped_encoder.layers.9.layer_norm.weight', 'encoder.wrapped_encoder.layers.3.final_layer_norm.bias', 'encoder.wrapped_encoder.layers.3.attention.out_proj.bias', 'speech_decoder_postnet.layers.3.conv.weight', 'speech_decoder_postnet.layers.0.batch_norm.num_batches_tracked', 'decoder.wrapped_decoder.layers.3.self_attn.q_proj.weight', 'decoder.wrapped_decoder.layers.5.feed_forward.intermediate_dense.bias', 'decoder.wrapped_decoder.layers.1.encoder_attn.out_proj.weight', 'encoder.wrapped_encoder.layers.1.attention.v_proj.bias', 'encoder.wrapped_encoder.layers.5.layer_norm.weight', 'encoder.wrapped_encoder.layers.2.attention.q_proj.weight', 'decoder.wrapped_decoder.layers.3.final_layer_norm.bias', 'decoder.wrapped_decoder.layers.4.self_attn.out_proj.bias', 'decoder.wrapped_decoder.layers.1.self_attn_layer_norm.weight', 'encoder.wrapped_encoder.layers.9.layer_norm.bias', 'encoder.wrapped_encoder.layers.11.layer_norm.bias', 'encoder.wrapped_encoder.layers.0.attention.k_proj.bias', 'encoder.wrapped_encoder.layers.9.attention.k_proj.weight', 'encoder.wrapped_encoder.layers.1.attention.out_proj.bias', 'encoder.wrapped_encoder.layers.4.feed_forward.intermediate_dense.weight', 'speech_decoder_postnet.feat_out.bias', 'decoder.wrapped_decoder.layers.5.encoder_attn.q_proj.bias', 'decoder.wrapped_decoder.layers.3.self_attn.q_proj.bias', 'decoder.wrapped_decoder.layers.4.encoder_attn.v_proj.bias', 'decoder.wrapped_decoder.layers.0.feed_forward.output_dense.weight', 'speech_decoder_postnet.layers.1.batch_norm.running_var', 'decoder.wrapped_decoder.layers.4.final_layer_norm.weight', 'decoder.wrapped_decoder.layers.0.self_attn_layer_norm.weight', 'decoder.wrapped_decoder.layers.1.encoder_attn.k_proj.weight', 'encoder.wrapped_encoder.layers.3.attention.k_proj.bias', 'encoder.wrapped_encoder.layers.9.attention.q_proj.bias', 'encoder.wrapped_encoder.layers.0.final_layer_norm.weight', 'encoder.wrapped_encoder.layers.8.final_layer_norm.bias', 'speech_decoder_postnet.layers.3.batch_norm.running_mean', 'encoder.wrapped_encoder.layers.7.feed_forward.output_dense.weight', 'encoder.wrapped_encoder.layers.2.attention.k_proj.weight', 'decoder.wrapped_decoder.layers.4.self_attn.q_proj.weight', 'encoder.wrapped_encoder.layers.10.final_layer_norm.bias', 'encoder.wrapped_encoder.layers.2.layer_norm.bias', 'decoder.wrapped_decoder.layers.0.self_attn.v_proj.bias', 'encoder.wrapped_encoder.layers.7.attention.q_proj.bias', 'encoder.wrapped_encoder.layers.10.layer_norm.bias', 'decoder.wrapped_decoder.layers.4.self_attn.k_proj.bias', 'encoder.wrapped_encoder.layers.5.final_layer_norm.weight', 'encoder.wrapped_encoder.layers.5.feed_forward.intermediate_dense.weight', 'encoder.wrapped_encoder.layers.3.attention.q_proj.bias', 'decoder.wrapped_decoder.layers.4.feed_forward.output_dense.weight', 'decoder.wrapped_decoder.layers.5.feed_forward.output_dense.bias', 'decoder.wrapped_decoder.layers.2.encoder_attn_layer_norm.bias', 'encoder.wrapped_encoder.layers.7.attention.v_proj.bias', 'speech_decoder_postnet.feat_out.weight', 'speech_decoder_postnet.layers.3.batch_norm.bias', 'encoder.wrapped_encoder.layers.9.feed_forward.intermediate_dense.weight', 'decoder.wrapped_decoder.layers.2.final_layer_norm.weight', 'encoder.wrapped_encoder.layers.9.feed_forward.output_dense.bias', 'encoder.wrapped_encoder.layers.4.attention.v_proj.weight', 'decoder.wrapped_decoder.layers.2.self_attn.v_proj.bias', 'encoder.wrapped_encoder.layers.5.attention.v_proj.weight', 'encoder.wrapped_encoder.layers.11.feed_forward.intermediate_dense.bias', 'encoder.wrapped_encoder.layers.11.final_layer_norm.weight', 'decoder.wrapped_decoder.layers.1.final_layer_norm.bias', 'encoder.wrapped_encoder.layers.2.attention.q_proj.bias', 'encoder.prenet.embed_tokens.weight', 'decoder.wrapped_decoder.layers.4.encoder_attn.k_proj.bias', 'decoder.wrapped_decoder.layers.5.self_attn.v_proj.weight', 'decoder.wrapped_decoder.layers.2.encoder_attn.out_proj.weight', 'encoder.wrapped_encoder.layers.8.feed_forward.intermediate_dense.bias', 'encoder.wrapped_encoder.layers.3.attention.out_proj.weight', 'encoder.wrapped_encoder.embed_positions.pe_k.weight', 'encoder.wrapped_encoder.layers.11.feed_forward.output_dense.weight', 'decoder.wrapped_decoder.layers.3.encoder_attn_layer_norm.bias', 'encoder.wrapped_encoder.layers.4.feed_forward.output_dense.bias', 'decoder.wrapped_decoder.layers.1.self_attn.q_proj.bias', 'decoder.wrapped_decoder.layers.5.encoder_attn_layer_norm.weight', 'encoder.wrapped_encoder.layers.11.attention.out_proj.bias', 'speech_decoder_postnet.layers.4.batch_norm.bias', 'speech_decoder_postnet.layers.4.batch_norm.num_batches_tracked', 'encoder.wrapped_encoder.layers.4.attention.q_proj.weight', 'encoder.wrapped_encoder.layers.8.attention.v_proj.weight', 'encoder.wrapped_encoder.layers.5.feed_forward.output_dense.weight', 'encoder.wrapped_encoder.layers.11.layer_norm.weight', 'decoder.wrapped_decoder.layers.3.self_attn.k_proj.weight', 'decoder.wrapped_decoder.layers.3.encoder_attn.out_proj.bias', 'encoder.wrapped_encoder.layers.9.attention.out_proj.weight', 'decoder.wrapped_decoder.layers.1.self_attn.v_proj.bias', 'decoder.wrapped_decoder.layers.0.feed_forward.output_dense.bias', 'decoder.wrapped_decoder.layers.5.self_attn.q_proj.bias', 'encoder.wrapped_encoder.layers.10.feed_forward.output_dense.weight', 'encoder.wrapped_encoder.layers.3.attention.k_proj.weight', 'encoder.wrapped_encoder.layers.7.feed_forward.intermediate_dense.weight', 'encoder.wrapped_encoder.layers.4.attention.q_proj.bias', 'encoder.wrapped_encoder.layers.10.attention.v_proj.weight', 'decoder.prenet.final_layer.weight', 'decoder.wrapped_decoder.layers.2.feed_forward.output_dense.bias', 'encoder.wrapped_encoder.layers.3.attention.q_proj.weight', 'encoder.wrapped_encoder.layers.11.attention.q_proj.weight', 'encoder.wrapped_encoder.layers.0.final_layer_norm.bias', 'encoder.wrapped_encoder.layers.4.attention.k_proj.weight', 'speech_decoder_postnet.layers.3.batch_norm.running_var', 'decoder.wrapped_decoder.layers.2.self_attn.k_proj.weight', 'encoder.wrapped_encoder.layers.6.attention.out_proj.weight', 'decoder.wrapped_decoder.layers.0.encoder_attn_layer_norm.bias', 'encoder.wrapped_encoder.layers.7.attention.v_proj.weight', 'encoder.wrapped_encoder.layers.0.layer_norm.weight', 'encoder.wrapped_encoder.layers.4.feed_forward.output_dense.weight', 'speech_decoder_postnet.layers.0.batch_norm.weight', 'encoder.wrapped_encoder.layers.1.attention.q_proj.weight', 'speech_decoder_postnet.layers.2.batch_norm.running_mean', 'encoder.wrapped_encoder.layers.2.final_layer_norm.bias', 'decoder.wrapped_decoder.layers.1.self_attn.k_proj.bias', 'encoder.wrapped_encoder.layers.3.layer_norm.bias', 'decoder.wrapped_decoder.layers.1.encoder_attn.out_proj.bias', 'decoder.wrapped_decoder.layers.3.self_attn.v_proj.bias', 'encoder.wrapped_encoder.layers.1.attention.v_proj.weight', 'encoder.wrapped_encoder.layers.7.final_layer_norm.bias', 'decoder.wrapped_decoder.layers.3.self_attn.out_proj.bias', 'decoder.wrapped_decoder.layers.2.encoder_attn_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the handler\n",
    "handler_obj.initialize(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock request data\n",
    "request_data = [{\"data\": \"Hi, My name is honey.\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the data\n",
    "preprocessed_data = handler_obj.preprocess(request_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(self, inputs):\n",
    "    inputs = inputs.to(self.device)\n",
    "    \n",
    "    # Assuming you have the embeddings_dataset loaded globally or load it here\n",
    "    embeddings_dataset = load_dataset(\"Matthijs/cmu-arctic-xvectors\", split=\"validation\", cache_dir=\"/Users/lhe339/Documents/dataset_cache\")\n",
    "    speaker_embeddings = torch.tensor(embeddings_dataset[7306][\"xvector\"]).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        speech = self.model.generate_speech(inputs[\"input_ids\"], speaker_embeddings, vocoder=self.vocoder)\n",
    "    return speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the handler object with the new inference method\n",
    "handler.SpeechSynthesisHandler.inference = inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference\n",
    "inference_output = handler_obj.inference(preprocessed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output.wav\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Postprocess the inference output\n",
    "postprocessed_output = handler_obj.postprocess(inference_output)\n",
    "\n",
    "# Print the result\n",
    "print(postprocessed_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
